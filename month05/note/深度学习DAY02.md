# æ·±åº¦å­¦ä¹ 

# TensorFlow

## ä»‹ç»

TensorFlowâ„¢ æ˜¯ä¸€ä¸ªé‡‡ç”¨æ•°æ®æµå›¾ï¼ˆdata flow graphsï¼‰ï¼Œç”¨äºæ•°å€¼è®¡ç®—çš„å¼€æºè½¯ä»¶åº“ã€‚èŠ‚ç‚¹ï¼ˆNodesï¼‰åœ¨å›¾ä¸­è¡¨ç¤ºæ•°å­¦æ“ä½œï¼Œå›¾ä¸­çš„çº¿ï¼ˆedgesï¼‰åˆ™è¡¨ç¤ºåœ¨èŠ‚ç‚¹é—´ç›¸äº’è”ç³»çš„å¤šç»´æ•°æ®æ•°ç»„ï¼Œå³å¼ é‡ï¼ˆtensorï¼‰ã€‚å®ƒçµæ´»çš„æ¶æ„è®©ä½ å¯ä»¥åœ¨å¤šç§å¹³å°ä¸Šå±•å¼€è®¡ç®—ï¼Œä¾‹å¦‚å°å¼è®¡ç®—æœºä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªCPUï¼ˆæˆ–GPUï¼‰ï¼ŒæœåŠ¡å™¨ï¼Œç§»åŠ¨è®¾å¤‡ç­‰ç­‰ã€‚

æ•°æ®æµå›¾ç”¨â€œç»“ç‚¹â€ï¼ˆnodesï¼‰å’Œâ€œçº¿â€(edges)çš„æœ‰å‘å›¾æ¥æè¿°æ•°å­¦è®¡ç®—ã€‚â€œèŠ‚ç‚¹â€ ä¸€èˆ¬ç”¨æ¥è¡¨ç¤ºæ–½åŠ çš„æ•°å­¦æ“ä½œï¼Œä½†ä¹Ÿå¯ä»¥è¡¨ç¤ºæ•°æ®è¾“å…¥ï¼ˆfeed inï¼‰çš„èµ·ç‚¹/è¾“å‡ºï¼ˆpush outï¼‰çš„ç»ˆç‚¹ï¼Œæˆ–è€…æ˜¯è¯»å–/å†™å…¥æŒä¹…å˜é‡ï¼ˆpersistent variableï¼‰çš„ç»ˆç‚¹ã€‚â€œçº¿â€è¡¨ç¤ºâ€œèŠ‚ç‚¹â€ä¹‹é—´çš„è¾“å…¥/è¾“å‡ºå…³ç³»ã€‚è¿™äº›æ•°æ®â€œçº¿â€å¯ä»¥è¾“è¿â€œsizeå¯åŠ¨æ€è°ƒæ•´â€çš„å¤šç»´æ•°æ®æ•°ç»„ï¼Œå³â€œå¼ é‡â€ï¼ˆtensorï¼‰ã€‚å¼ é‡ä»å›¾ä¸­æµè¿‡çš„ç›´è§‚å›¾åƒæ˜¯è¿™ä¸ªå·¥å…·å–åä¸ºâ€œTensorflowâ€çš„åŸå› ã€‚ä¸€æ—¦è¾“å…¥ç«¯çš„æ‰€æœ‰å¼ é‡å‡†å¤‡å¥½ï¼ŒèŠ‚ç‚¹å°†è¢«åˆ†é…åˆ°å„ç§è®¡ç®—è®¾å¤‡å®Œæˆå¼‚æ­¥å¹¶è¡Œåœ°æ‰§è¡Œè¿ç®—ã€‚

ç”¨å¼ é‡è¡¨ç¤ºæ•°æ®ï¼Œç”¨è®¡ç®—å›¾æ­å»ºç¥ç»ç½‘ç»œï¼Œç”¨ä¼šè¯æ‰§è¡Œè®¡ç®—å›¾ï¼Œä¼˜åŒ–çº¿ä¸Šçš„æƒé‡ï¼ˆå‚æ•°ï¼‰ï¼Œå¾—åˆ°æ¨¡å‹ã€‚

http://playground.tensorflow.org/

### TensorFlowè®¡ç®—æ¨¡å‹â€”â€”è®¡ç®—å›¾

è®¡ç®—å›¾â€”â€”æ­å»ºç¥ç»ç½‘ç»œçš„è®¡ç®—è¿‡ç¨‹ï¼Œåªæ­å»ºï¼Œä¸è¿ç®—ã€‚

TensorFlow çš„åå­—ä¸­å·±ç»è¯´æ˜äº†å®ƒæœ€é‡è¦çš„ä¸¤ä¸ªæ¦‚å¿µä¸€ä¸€Tensor å’ŒFlow ã€‚Tensor å°±æ˜¯å¼ é‡ã€‚å¼ é‡è¿™ä¸ªæ¦‚å¿µåœ¨æ•°å­¦æˆ–è€…ç‰©ç†å­¦ä¸­å¯ä»¥æœ‰ä¸åŒçš„è§£é‡Šã€‚åœ¨TensorFlow ä¸­ï¼Œå¼ é‡å¯ä»¥è¢«ç®€å•åœ°ç†è§£ä¸ºå¤šç»´æ•°ç»„ï¼Œå¦‚æœè¯´TensorFlow çš„ç¬¬ä¸€ä¸ªè¯Tensor è¡¨æ˜äº†å®ƒçš„æ•°æ®ç»“æ„ï¼Œé‚£ä¹ˆFlow åˆ™ä½“ç°äº†å®ƒçš„è®¡ç®—æ¨¡å‹ã€‚Flow ç¿»è¯‘æˆä¸­æ–‡å°±æ˜¯â€œæµâ€ï¼Œå®ƒç›´è§‚åœ°è¡¨è¾¾äº†å¼ é‡ä¹‹é—´é€šè¿‡è®¡ç®—ç›¸äº’è½¬åŒ–çš„è¿‡ç¨‹ã€‚

TensorFlow æ˜¯ä¸€ä¸ªé€šè¿‡è®¡ç®—å›¾çš„å½¢å¼æ¥è¡¨è¿°è®¡ç®—çš„ç¼–ç¨‹ç³»ç»Ÿã€‚TensorFlowä¸­çš„æ¯ä¸€ä¸ªè®¡ç®—éƒ½æ˜¯è®¡ç®—å›¾ä¸Šçš„ä¸€ä¸ªèŠ‚ç‚¹ï¼Œè€ŒèŠ‚ç‚¹ä¹‹é—´çš„è¾¹æè¿°äº†è®¡ç®—ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

![1563504702363](images/1563504702363.png)

TensorFlow ç¨‹åºä¸€èˆ¬å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚

åœ¨ç¬¬ä¸€ä¸ªé˜¶æ®µéœ€è¦å®šä¹‰è®¡ç®—å›¾ä¸­æ‰€æœ‰çš„è®¡ç®—ã€‚
æ¯”å¦‚åœ¨ä¸Šå›¾å‘é‡åŠ æ³•æ ·ä¾‹ç¨‹åºä¸­é¦–å…ˆå®šä¹‰äº†ä¸¤ä¸ªè¾“å…¥ï¼Œç„¶åå®šä¹‰äº†ä¸€ä¸ªè®¡ç®—æ¥å¾—åˆ°å®ƒä»¬çš„å’Œã€‚

ç¬¬äºŒä¸ªé˜¶æ®µä¸ºæ‰§è¡Œè®¡ç®—ã€‚

```python
import tensorflow as tf
a = tf.constant([1.0, 2.0], name='a')
b = tf.constant([2.0, 3.0], name='b')
result = a + b
```

### **TensorFlowæ•°æ®æ¨¡å‹â€”â€”å¼ é‡ï¼ˆtf.Tensorï¼‰**

https://tensorflow.google.cn/guide/tensors

å¼ é‡â€”â€”å¤šç»´æ•°ç»„ï¼ˆåˆ—è¡¨ï¼‰ï¼Œé˜¶â€”â€”å¼ é‡çš„ç»´æ•°ã€‚

åœ¨TensorFlowç¨‹åºä¸­ï¼Œæ‰€æœ‰çš„æ•°æ®éƒ½é€šè¿‡å¼ é‡çš„å½¢å¼æ¥è¡¨ç¤ºã€‚ä»åŠŸèƒ½çš„è§’åº¦ä¸Šçœ‹ï¼Œå¼ é‡å¯ä»¥è¢«ç®€å•ç†è§£ä¸º**å¤šç»´æ•°ç»„**ã€‚å…¶ä¸­é›¶é˜¶å¼ é‡è¡¨ç¤ºæ ‡é‡ï¼ˆ scalar ï¼‰ ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªæ•° ï¼› ç¬¬ä¸€é˜¶å¼ é‡ä¸ºå‘é‡ï¼ˆ vector),ä¹Ÿå°±æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼›ç¬¬n é˜¶å¼ é‡å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªn ç»´æ•°ç»„ã€‚

| é˜¶   | æ•°å­¦å®ä¾‹           |                             |
| ---- | ------------------ | --------------------------- |
| 0    | æ ‡é‡ï¼ˆåªæœ‰å¤§å°ï¼‰   | s=123                       |
| 1    | çŸ¢é‡ï¼ˆå¤§å°å’Œæ–¹å‘ï¼‰ | v=[1,2,3]                   |
| 2    | çŸ©é˜µï¼ˆæ•°æ®è¡¨ï¼‰     | m=[[1,2,3],[4,5,6],[7,8,9]] |
| n    | n é˜¶å¼ é‡           | t=[[[[...]]]]      nä¸ª[]    |

ä½†å¼ é‡åœ¨Tensor Flow ä¸­çš„å®ç°å¹¶ä¸æ˜¯ç›´æ¥é‡‡ç”¨æ•°ç»„çš„å½¢å¼ï¼Œå®ƒåªæ˜¯å¯¹TensorFlow ä¸­è¿ç®—ç»“æœçš„å¼•ç”¨ã€‚åœ¨å¼ é‡ä¸­å¹¶æ²¡æœ‰çœŸæ­£ä¿å­˜æ•°å­—ï¼Œå®ƒä¿å­˜çš„æ˜¯å¦‚ä½•å¾—åˆ°è¿™äº›æ•°å­—çš„è®¡ç®—è¿‡ç¨‹ã€‚è¿˜æ˜¯ä»¥å‘é‡åŠ æ³•ä¸ºä¾‹ï¼Œå½“è¿è¡Œå¦‚ä¸‹ä»£ç æ—¶ï¼Œå¹¶ä¸ä¼šå¾—åˆ°åŠ æ³•çš„ç»“æœï¼Œè€Œä¼šå¾—åˆ°å¯¹ç»“æœçš„ä¸€ä¸ªå¼•ç”¨ã€‚

```python
import tensorflow as tf
# tf.constant æ˜¯ä¸€ä¸ªè®¡ç®—ï¼Œè¿™ä¸ªè®¡ç®—çš„ç»“æœä¸ºä¸€ä¸ªå¼ é‡ï¼Œ ä¿å­˜åœ¨å˜é‡a ä¸­ã€‚
a = tf.constant([1.0,2.0],name ='a')
b = tf.constant([2.0,3.0], name ='b')
result = tf.add(a,b, name='add')
print(result)
'''
è¾“å‡ºï¼š
Tensor("add:0", shape=(2,), dtype=float32)
'''
```

ä»ä¸Šé¢ä»£ç çš„è¿è¡Œç»“æœå¯ä»¥çœ‹å‡ºï¼Œ ä¸€ä¸ªå¼ é‡ä¸­ä¸»è¦ä¿å­˜äº†ä¸‰ä¸ªå±æ€§ï¼š 

**åå­—ï¼ˆ name ï¼‰ã€ç»´åº¦ï¼ˆ shape ï¼‰å’Œç±»å‹ï¼ˆ type ï¼‰**ã€‚

æˆ‘ä»¬ä»ä¸Šå›¾äº†è§£äº†Tensor Flow çš„è®¡ç®—éƒ½å¯ä»¥é€šè¿‡è®¡ç®—å›¾çš„æ¨¡å‹æ¥å»ºç«‹ï¼Œ
è€Œè®¡ç®—å›¾ä¸Šçš„æ¯ä¸€ä¸ªèŠ‚ç‚¹ä»£è¡¨äº†ä¸€ä¸ªè®¡ç®—ï¼Œè®¡ç®—çš„ç»“æœå°±ä¿å­˜åœ¨å¼ é‡ä¹‹ä¸­ã€‚æ‰€ä»¥å¼ é‡å’Œè®¡ç®—å›¾ä¸ŠèŠ‚ç‚¹æ‰€ä»£è¡¨çš„è®¡ç®—ç»“æœæ˜¯å¯¹åº”çš„ã€‚è¿™æ ·å¼ é‡çš„å‘½åå°±å¯ä»¥é€šè¿‡â€œ **node:src_ output** â€çš„å½¢å¼æ¥ç»™å‡ºã€‚å…¶ä¸­node ä¸ºèŠ‚ç‚¹çš„åç§°ï¼Œ srcä¸€output è¡¨ç¤ºå½“å‰å¼ é‡æ¥è‡ªèŠ‚ç‚¹çš„ç¬¬å‡ ä¸ªè¾“å‡ºã€‚
æ¯”å¦‚ä¸Šé¢ä»£ç æ‰“å‡ºæ¥çš„ "add:0" å°±è¯´æ˜äº†result è¿™ä¸ªå¼ é‡æ˜¯è®¡ç®—èŠ‚ç‚¹â€œ add â€ è¾“å‡ºçš„ç¬¬ä¸€ä¸ªç»“æœï¼ˆç¼–å·ä»0å¼€å§‹ï¼‰ã€‚

å¼ é‡çš„ç¬¬äºŒä¸ªå±æ€§æ˜¯å¼ é‡çš„ç»´åº¦ï¼ˆ shape ï¼‰ã€‚è¿™ä¸ªå±æ€§æè¿°äº†ä¸€ä¸ªå¼ é‡çš„ç»´åº¦ä¿¡æ¯ã€‚æ¯”å¦‚ä¸Šé¢æ ·ä¾‹ä¸­ shape = ( 2,  ) è¯´æ˜äº†å¼ é‡result æ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œ è¿™ä¸ªæ•°ç»„çš„é•¿åº¦ä¸º2 ã€‚

å¼ é‡çš„ç¬¬ä¸‰ä¸ªå±æ€§æ˜¯ç±»å‹ï¼ˆ type ï¼‰ï¼Œæ¯ä¸€ä¸ªå¼ é‡ä¼šæœ‰ä¸€ä¸ªå”¯ä¸€çš„ç±»å‹ã€‚TensorFlow ä¼šå¯¹å‚ä¸è¿ç®—çš„æ‰€æœ‰å¼ é‡è¿›è¡Œç±»å‹çš„æ£€æŸ¥ï¼Œ å½“å‘ç°ç±»å‹ä¸åŒ¹é…æ—¶ä¼šæŠ¥é”™ã€‚

TensorFlow æ”¯æŒ14 ç§ä¸åŒçš„ç±»å‹ï¼Œ ä¸»è¦åŒ…æ‹¬äº†å®æ•°ï¼ˆ tf.float32 ã€tf.float64 ï¼‰ã€æ•´æ•°ï¼ˆ tf.int8 ã€tf.intl6 ã€tf.int32 ã€tf.int64 ã€tf.uint8 ï¼‰ã€å¸ƒå°”å‹ï¼ˆ tf.bool) å’Œå¤æ•°ï¼ˆ tf.complex64 ã€tf.complex128 ï¼‰ ã€‚



### TensorFlowè¿è¡Œæ¨¡å‹â€”â€”ä¼šè¯ï¼ˆsessionï¼‰

æ‰§è¡Œè®¡ç®—å›¾ä¸­çš„èŠ‚ç‚¹è¿ç®—ã€‚

å‰é¢çš„ä¸¤èŠ‚ä»‹ç»äº†TensorFlow æ˜¯å¦‚ä½•ç»„ç»‡æ•°æ®å’Œè¿ç®—çš„ã€‚æœ¬èŠ‚å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨
TensorFlow ä¸­çš„ä¼šè¯ï¼ˆ session ï¼‰æ¥æ‰§è¡Œå®šä¹‰å¥½çš„è¿ç®—ã€‚ä¼šè¯æ‹¥æœ‰å¹¶ç®¡ç†TensorFlow ç¨‹åºè¿è¡Œæ—¶çš„æ‰€æœ‰èµ„æºã€‚æ‰€æœ‰è®¡ç®—å®Œæˆä¹‹åéœ€è¦å…³é—­ä¼šè¯æ¥å¸®åŠ©ç³»ç»Ÿå›æ”¶èµ„æºï¼Œå¦åˆ™å°±å¯èƒ½å‡ºç°èµ„æºæ³„æ¼çš„é—®é¢˜ã€‚TensorFlow ä¸­ä½¿ç”¨ä¼šè¯çš„æ¨¡å¼ä¸€èˆ¬æœ‰ä¸¤ç§ï¼Œç¬¬ä¸€ç§æ¨¡å¼éœ€è¦æ˜ç¡®è°ƒç”¨ä¼šè¯ç”Ÿæˆå‡½æ•°å’Œå…³é—­ä¼šè¯å‡½æ•°ï¼Œè¿™ç§æ¨¡å¼çš„ä»£ç æµç¨‹å¦‚ä¸‹ã€‚

```python
# åˆ›å»ºä¸€ä¸ªä¼šè¯ã€‚
sess = tf.Session()

#ä½¿ç”¨è¿™ä¸ªåˆ›å»ºå¥½çš„ä¼šè¯æ¥å¾—åˆ°å…³å¿ƒçš„è¿ç®—çš„ç»“æœã€‚æ¯”å¦‚å¯ä»¥è°ƒç”¨sess.run(result),
# æ¥å¾—åˆ°ä¸Šè¿°ä¾‹ä¸­å¼ é‡result çš„å–å€¼ã€‚
print(sess.run(result))

# å…³é—­ä¼šè¯ä½¿å¾—æœ¬æ¬¡è¿è¡Œä¸­ä½¿ç”¨åˆ°çš„èµ„æºå¯ä»¥è¢«é‡Šæ”¾ã€‚
sess.close()
```

ä½¿ç”¨è¿™ç§æ¨¡å¼æ—¶ï¼Œåœ¨æ‰€æœ‰è®¡ç®—å®Œæˆä¹‹åï¼Œéœ€è¦æ˜ç¡®è°ƒç”¨Session.close å‡½æ•°æ¥å…³é—­ä¼šè¯å¹¶é‡Šæ”¾èµ„æºã€‚

ä¸ºäº†è§£å†³å¼‚å¸¸é€€å‡ºæ—¶èµ„æºé‡Šæ”¾çš„é—®é¢˜ï¼Œ TensorFlow å¯ä»¥é€šè¿‡Python çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥ä½¿ç”¨ä¼šè¯ã€‚ä»¥ä¸‹ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è¿™ç§æ¨¡å¼ã€‚

```python
#åˆ›å»ºä¸€ä¸ªä¼šè¯ï¼Œå¹¶é€šè¿‡Python ä¸­çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥ç®¡ç†è¿™ä¸ªä¼šè¯ã€‚
with tf.Session() as sess:
#ä½¿ç”¨åˆ›å»ºå¥½çš„ä¼šè¯æ¥è®¡ç®—å…³å¿ƒçš„ç»“æœã€‚
  print(sess.run(result))
#ä¸éœ€è¦å†è°ƒç”¨" Session.close()"å‡½æ•°æ¥å…³é—­ä¼šè¯ï¼Œ
#å½“ä¸Šä¸‹æ–‡é€€å‡ºæ—¶ä¼šè¯å…³é—­å’Œèµ„æºé‡Šæ”¾ä¹Ÿè‡ªåŠ¨å®Œæˆäº†ã€‚
```

é€šè¿‡Python ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„æœºåˆ¶ï¼Œåªè¦å°†æ‰€æœ‰çš„è®¡ç®—æ”¾åœ¨â€œ with â€çš„å†…éƒ¨å°±å¯ä»¥ã€‚å½“ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºæ—¶å€™ä¼šè‡ªåŠ¨é‡Šæ”¾æ‰€æœ‰èµ„æºã€‚è¿™æ ·æ—¢è§£å†³äº†å› ä¸ºå¼‚å¸¸é€€å‡ºæ—¶èµ„æºé‡Šæ”¾çš„é—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿè§£å†³äº†å¿˜è®°è°ƒç”¨Session.close å‡½æ•°è€Œäº§ç”Ÿçš„èµ„æºæ³„æ¼ã€‚

### å˜é‡ï¼ˆ tf. Variable ï¼‰

å˜é‡ï¼ˆ tf. Variable ï¼‰çš„ä½œç”¨å°±æ˜¯ä¿å­˜å’Œæ›´æ–°ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°ã€‚

å‚æ•°ï¼šå³çº¿ä¸Šçš„æƒé‡Wï¼Œç”¨å˜é‡è¡¨ç¤ºï¼Œéšæœºç»™åˆå€¼ã€‚

å½“åˆ›å»ºä¸€ä¸ªå˜é‡æ—¶ï¼Œä½ å°†ä¸€ä¸ªå¼ é‡ä½œä¸ºåˆå§‹å€¼ä¼ å…¥æ„é€ å‡½æ•°Variable()ã€‚TensorFlowæä¾›äº†ä¸€ç³»åˆ—æ“ä½œç¬¦æ¥åˆå§‹åŒ–å¼ é‡ï¼Œåˆå§‹å€¼æ˜¯å¸¸é‡æˆ–æ˜¯éšæœºå€¼ã€‚

æ³¨æ„ï¼Œæ‰€æœ‰è¿™äº›æ“ä½œç¬¦éƒ½éœ€è¦ä½ æŒ‡å®šå¼ é‡çš„shapeã€‚é‚£ä¸ªå½¢çŠ¶è‡ªåŠ¨æˆä¸ºå˜é‡çš„shapeã€‚å˜é‡çš„shapeé€šå¸¸æ˜¯å›ºå®šçš„ï¼Œä½†TensorFlowæä¾›äº†é«˜çº§çš„æœºåˆ¶æ¥é‡æ–°è°ƒæ•´å…¶è¡Œåˆ—æ•°ã€‚

```python
# ç”Ÿæˆ200ä¸ªå…¨0å…ƒç´ çš„å¼ é‡
biases = tf.Variable(tf.zeros([200]), name="biases")

#æ­£æ€åˆ†å¸ƒï¼Œäº§ç”Ÿ784*200çš„çŸ©é˜µï¼Œæ ‡å‡†å·®ä¸º0.35ï¼Œå‡å€¼ä¸º0.éšæœºç§å­
weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35,mean=0, seed=1),name="weights")
```

ç¥ç»ç½‘ç»œä¸­å¸¸ç”¨çš„ç”Ÿæˆéšæœºæ•°æ•°ç»„çš„å‡½æ•°æœ‰ï¼š

| å‡½æ•°                  | æè¿°                               |
| --------------------- | ---------------------------------- |
| tf.random_normal()    | ç”Ÿæˆæ­£æ€åˆ†å¸ƒéšæœºæ•°                 |
| tf.truncated_normal() | ç”Ÿæˆå»æ‰è¿‡å¤§åç¦»ç‚¹çš„æ­£æ€åˆ†å¸ƒéšæœºæ•° |
| tf.random_uniform()   | ç”Ÿæˆå‡åŒ€åˆ†å¸ƒéšæœºæ•°                 |
| tf.zeros              | è¡¨ç¤ºç”Ÿæˆå…¨ 0 æ•°ç»„                  |
| tf.ones               | è¡¨ç¤ºç”Ÿæˆå…¨ 1 æ•°ç»„                  |
| tf.fill               | è¡¨ç¤ºç”Ÿæˆå…¨å®šå€¼æ•°ç»„                 |
| tf.constant           |                                    |

å˜é‡çš„åˆå§‹åŒ–å¿…é¡»åœ¨æ¨¡å‹çš„å…¶å®ƒæ“ä½œè¿è¡Œä¹‹å‰å…ˆæ˜ç¡®åœ°å®Œæˆã€‚æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯æ·»åŠ ä¸€ä¸ªç»™æ‰€æœ‰å˜é‡åˆå§‹åŒ–çš„æ“ä½œï¼Œå¹¶åœ¨ä½¿ç”¨æ¨¡å‹ä¹‹å‰é¦–å…ˆè¿è¡Œé‚£ä¸ªæ“ä½œã€‚

```python
# Create two variables.
weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),name="weights")
biases = tf.Variable(tf.zeros([200]), name="biases")
...
# Add an op to initialize the variables.
init_op = tf.initialize_all_variables()

# Later, when launching the model
with tf.Session() as sess:
  # Run the init operation.
  sess.run(init_op)
  ...
  # Use the model
  ...
```

## ç¥ç»ç½‘ç»œçš„å®ç°è¿‡ç¨‹

1ã€å‡†å¤‡æ•°æ®é›†ï¼Œæå–ç‰¹å¾ï¼Œä½œä¸ºè¾“å…¥å–‚ç»™ç¥ç»ç½‘ç»œï¼ˆ Neural Network NN)
2ã€æ­å»º NN ç»“æ„ï¼Œä»è¾“å…¥åˆ°è¾“å‡ºï¼ˆå…ˆæ­å»ºè®¡ç®—å›¾ï¼Œå†ç”¨ä¼šè¯æ‰§è¡Œï¼‰
3ã€å¤§é‡ç‰¹å¾æ•°æ®å–‚ç»™ NN ï¼Œè¿­ä»£ä¼˜åŒ– NN å‚æ•°
4ã€ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹å’Œåˆ†ç±»

### åŸºäºtensorflowçš„å‰å‘ä¼ æ’­

å˜é‡åˆå§‹åŒ–ï¼šåœ¨ sess.run å‡½æ•°ä¸­ç”¨ tf.global_variables_initializer() æ±‡æ€»æ‰€æœ‰å¾…ä¼˜åŒ–å˜é‡ã€‚

```python
init_op = tf.global_variables_initializer()
sess.run(init_op)
```

è®¡ç®—å›¾èŠ‚ç‚¹è¿ç®—ï¼šåœ¨sess.runå‡½æ•°ä¸­å†™å…¥å¾…è¿ç®—çš„èŠ‚ç‚¹

```python
sess.run(y)
```

ç”¨ tf.placeholderå ä½ï¼Œåœ¨ sess.run å‡½æ•°ä¸­ç”¨å‡½æ•°ä¸­ç”¨ feed_dictå–‚æ•°æ®

```python
with tf.Session() as sess:
	#å–‚ä¸€ç»„æ•°æ®ï¼š
	x = tf.placeholder(tf.float32, shape=(1, 2))
	y = x + x
	r = sess.run(y, feed_dict={x: [[0.5,0.6]]})
	print(r)
	#å–‚å¤šç»„æ•°æ®ï¼š
	x = tf.placeholder(tf.float32, shape=(None, 2))
	y = tf.reduce_sum(x, 0)
	r = sess.run(y, feed_dict={x: [[0.1,0.2],[0.2,0.3],[0.3,0.4],[0.4,0.5]]})
	print(r)
```

## åå‘ä¼ æ’­

åå‘ä¼ æ’­ ï¼šè®­ç»ƒæ¨¡å‹å‚æ•° ï¼Œåœ¨æ‰€æœ‰å‚æ•°ä¸Šç”¨æ¢¯åº¦ä¸‹é™ï¼Œä½¿ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æŸå¤±å‡½æ•°æœ€å°ã€‚

### æŸå¤±å‡½æ•°

æŸå¤±å‡½æ•°çš„è®¡ç®—æœ‰å¾ˆå¤šæ–¹æ³•ã€‚

#### è§£å†³å›å½’é—®é¢˜çš„æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·®MSE

![1563865732862](images/1563865732862.png)



ç”¨tensorflow å‡½æ•°è¡¨ç¤ºä¸ºloss_mse = tf.reduce_mean(tf.square(y_ - y))

åå‘ä¼ æ’­è®­ç»ƒæ–¹æ³•ï¼š ä»¥å‡å° loss å€¼ä¸ºä¼˜åŒ–ç›®æ ‡ ï¼Œæœ‰æ¢¯åº¦ä¸‹é™ ã€  adamä¼˜åŒ–å™¨ç­‰ä¼˜åŒ–æ–¹æ³•ã€‚

è¿™ä¸¤ç§ä¼˜åŒ–æ–¹æ³•ç”¨tensorflow çš„å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š

```python
train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)

train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)
```



1. tf.train.GradientDescentOptimizer ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½¿å‚æ•°æ²¿ç€
   æ¢¯åº¦çš„åæ–¹å‘ï¼Œå³æ€»æŸå¤±å‡å°çš„æ–¹å‘ç§»åŠ¨ï¼Œå®ç°æ›´æ–°å‚æ•°ã€‚

   å…¶ä¸­ï¼Œğ½(ğœƒ)ä¸ºæŸå¤±å‡½æ•°ï¼Œ ğœƒä¸ºå‚æ•°ï¼Œ ğ›¼ä¸ºå­¦ä¹ ç‡ã€‚

   ![1563866296204](images/1563866296204.png)

2. tf.train.AdamOptimizer() æ˜¯åˆ©ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜åŒ–ç®—æ³•ï¼Œ Adam ç®—æ³•å’Œéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ä¸åŒã€‚éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ä¿æŒå•ä¸€çš„å­¦ä¹ ç‡æ›´æ–°æ‰€æœ‰çš„å‚æ•°ï¼Œå­¦ä¹ ç‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹¶ä¸ä¼šæ”¹å˜ã€‚è€Œ Adam ç®—æ³•é€šè¿‡è®¡ç®—æ¢¯åº¦çš„ä¸€é˜¶çŸ©ä¼°è®¡å’ŒäºŒé˜¶çŸ©ä¼°è®¡è€Œä¸ºä¸åŒçš„å‚æ•°è®¾è®¡ç‹¬ç«‹çš„è‡ªé€‚åº”æ€§å­¦ä¹ ç‡ã€‚

å­¦ä¹ ç‡ learning_rateï¼š å†³å®šæ¯æ¬¡å‚æ•°æ›´æ–°çš„å¹…åº¦ã€‚
ä¼˜åŒ–å™¨ä¸­éƒ½éœ€è¦ä¸€ä¸ªå«åšå­¦ä¹ ç‡çš„å‚æ•°ï¼Œä½¿ç”¨æ—¶å¦‚æœå­¦ä¹ ç‡é€‰æ‹©è¿‡å¤§ä¼šå¯¼è‡´å¾…ä¼˜åŒ–çš„å‚æ•°åœ¨æœ€å°å€¼é™„è¿‘æ³¢åŠ¨ä¸æ”¶æ•›çš„æƒ…å†µï¼Œå¦‚æœå­¦ä¹ ç‡é€‰æ‹©è¿‡å°ï¼Œä¼šå‡ºç°æ”¶æ•›é€Ÿåº¦æ…¢çš„æƒ…å†µã€‚ æˆ‘ä»¬å¯ä»¥é€‰ä¸ªæ¯”è¾ƒå°çš„å€¼å¡«å…¥ï¼Œ æ¯”å¦‚ 0.01 ã€ 0.001ã€‚

#### è§£å†³åˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µï¼ˆ cross entropy ï¼‰

å‡è®¾æœ‰ä¸¤ä¸ªåˆ†å¸ƒpï¼ˆ1, 0, 0ï¼‰ä¸ qï¼ˆ0.8, 0.1, 0.1ï¼‰ï¼Œåˆ™å®ƒä»¬åœ¨ç»™å®šæ ·æœ¬é›†ä¸Šçš„äº¤å‰ç†µå®šä¹‰å¦‚ä¸‹ï¼š 
$$
CE(p,q)=âˆ’\sum_{}p(x)logq(x)
$$

------

ç”¨Tensorflow å‡½æ•°è¡¨ç¤º

```python
ce=-tf.reduce_sum(p * tf.log(tf.clip_by_value(q, 1e-12, 1.0))) 
```

ï¼ˆ1e-12 æ˜¯ä¸ºäº†é˜²æ­¢log0å‡ºç°ï¼‰

ä¸¤ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹è§£å†³äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œå·²çŸ¥æ ‡å‡†ç­”æ¡ˆä¸ºp = (1, 0)ï¼Œç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹ç»“æœä¸ºq1=(0.6, 0.4)ï¼Œç¬¬äºŒä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹ç»“æœä¸ºq2=(0.8, 0.2)ï¼Œåˆ¤æ–­å“ªä¸ªç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹çš„ç»“æœæ›´æ¥è¿‘æ ‡å‡†ç­”æ¡ˆã€‚
æ ¹æ®äº¤å‰ç†µçš„è®¡ç®—å…¬å¼å¾—ï¼š

```
H1((1,0),(0.6,0.4)) = -(1*log0.6 + 0*log0.4) â‰ˆâ‰ˆ -(-0.222 + 0) = 0.222
H2((1,0),(0.8,0.2)) = -(1*log0.8 + 0*log0.2) â‰ˆâ‰ˆ -(-0.097 + 0) = 0.097
```

ç”±äº0.222>0.097ï¼Œæ‰€ä»¥é¢„æµ‹ç»“æœy2ä¸æ ‡å‡†ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆy_æ›´æ¥è¿‘ï¼Œy2é¢„æµ‹æ›´å‡†ç¡®ã€‚

**æ€»ç»“ï¼š**

äº¤å‰ç†µåˆ»ç”»äº†ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œ å®ƒæ˜¯åˆ†ç±»é—®é¢˜ä¸­ä½¿ç”¨æ¯”è¾ƒå¹¿çš„ä¸€ç§æŸå¤±å‡½æ•°ã€‚

äº¤å‰ç†µè¶Šå¤§ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè·ç¦»è¶Šè¿œï¼Œ ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè¶Šç›¸å¼‚ ;

äº¤å‰ç†µè¶Šå°ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè·ç¦»è¶Šè¿‘ ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒè¶Šç›¸ä¼¼ ã€‚



TensorFlowé’ˆå¯¹åˆ†ç±»é—®é¢˜ï¼Œå®ç°äº†å¸¸è§çš„äº¤å‰ç†µå‡½æ•°ï¼Œåˆ†åˆ«æ˜¯

- **tf.nn.sigmoid_cross_entropy_with_logits**

```python
tf.nn.sigmoid_cross_entropy_with_logits(logits=é¢„æµ‹è¾“å‡º, labels=çœŸå®è¾“å‡º)
```

è®¡ç®—æ–¹å¼ï¼šå¯¹è¾“å…¥çš„y'å…ˆé€šè¿‡sigmoidå‡½æ•°è®¡ç®—ï¼Œå†è®¡ç®—å®ƒä»¬çš„äº¤å‰ç†µï¼Œä½†æ˜¯å®ƒå¯¹äº¤å‰ç†µçš„è®¡ç®—æ–¹å¼è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å¾—å‡ºçš„ç»“æœä¸è‡³äºæº¢å‡ºã€‚

é€‚ç”¨ï¼šæ¯ä¸ªç±»åˆ«ç›¸äº’ç‹¬ç«‹ä½†äº’ä¸æ’æ–¥çš„æƒ…å†µï¼šä¾‹å¦‚ä¸€å¹…å›¾å¯ä»¥åŒæ—¶åŒ…å«ä¸€æ¡ç‹—å’Œä¸€åªå¤§è±¡ã€‚

outputä¸æ˜¯ä¸€ä¸ªæ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªbatchä¸­æ¯ä¸ªæ ·æœ¬çš„loss,æ‰€ä»¥ä¸€èˆ¬é…åˆ`tf.reduce_mean(loss)`ä½¿ç”¨ã€‚

```python
import tensorflow as tf
import numpy as np

# 5ä¸ªæ ·æœ¬ä¸‰åˆ†ç±»é—®é¢˜ï¼Œä¸”ä¸€ä¸ªæ ·æœ¬å¯ä»¥åŒæ—¶æ‹¥æœ‰å¤šç±»
y = np.array([[1,0,0],[1,0,0]], dtype='f8')
y_ = np.array([[12,3,2],[3,10,1]], dtype='f8')
sess =tf.Session()
error = sess.run(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_))
print(error) 
print(error.mean(axis=1)) 
sess.close()
```

- **tf.nn.softmax_cross_entropy_with_logits**

```python
tf.nn.softmax_cross_entropy_with_logits(labels=None, logits=None)
```

è®¡ç®—æ–¹å¼ï¼šå¯¹è¾“å…¥çš„logitså…ˆé€šè¿‡softmaxå‡½æ•°è®¡ç®—ï¼Œå†è®¡ç®—å®ƒä»¬çš„äº¤å‰ç†µï¼Œä½†æ˜¯å®ƒå¯¹äº¤å‰ç†µçš„è®¡ç®—æ–¹å¼è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å¾—ç»“æœä¸è‡³äºæº¢å‡ºã€‚

é€‚ç”¨ï¼šæ¯ä¸ªç±»åˆ«ç›¸äº’ç‹¬ç«‹ä¸”æ’æ–¥çš„æƒ…å†µï¼Œä¸€å¹…å›¾åªèƒ½å±äºä¸€ç±»ï¼Œè€Œä¸èƒ½åŒæ—¶åŒ…å«ä¸€æ¡ç‹—å’Œä¸€åªå¤§è±¡ã€‚

outputï¼šä¸æ˜¯ä¸€ä¸ªæ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªbatchä¸­æ¯ä¸ªæ ·æœ¬çš„lossï¼Œæ‰€ä»¥ä¸€èˆ¬é…åˆ`tf.reduce_mean(loss)`ä½¿ç”¨ã€‚

```python
import tensorflow as tf
import numpy as np

y = np.array([[1,0,0],[1,0,0]], dtype='f8')# æ¯ä¸€è¡Œåªæœ‰ä¸€ä¸ª1
y_ =np.array([[12,3,2],[3,10,1]], dtype='f8')
sess = tf.Session()
error = sess.run(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_))
print(error)
```



## ç¥ç»ç½‘ç»œè®¡ç®—è¿‡ç¨‹ï¼š

1. å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†ï¼›
   import
   å¸¸é‡å®šä¹‰
   ç”Ÿæˆæ•°æ®é›†

2. å‰å‘ä¼ æ’­ï¼šå®šä¹‰è¾“å…¥ã€å‚æ•°å’Œè¾“å‡º
   x =              y =
   w1 =           w2 =
   b1 =            b2 = 

   y_ = 
   
3. åå‘ä¼ æ’­ï¼šå®šä¹‰æŸå¤±å‡½æ•°ã€åå‘ä¼ æ’­æ–¹æ³•
   loss = 
   train_step = 

4. ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒ STEPS è½®

   ```python
   with tf.session() as sess:
       init_op=tf.global_variables_initializer()
       sess.run(init_op)
       STEPS=3000
       for i in range(STEPS):
           start=
           end=
           sess.run(train_step, feed_dict:)
   ```

### åŸºäºtensorflowè®­ç»ƒç¥ç»ç½‘ç»œ

```python
#coding utf-8
#å¯¼å…¥æ¨¡å—ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†
import tensorflow as tf
import numpy as np
import sklearn.datasets as datasets
import matplotlib.pyplot as mp
BATCH_SIZE = 8
seed =23455

np.random.seed(0)
X, Y = datasets.make_moons(200, noise=0.10)
Y = np.array(np.column_stack((Y, ~Y+2)), dtype='f4')
print(Y)

#å®šä¹‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ã€å‚æ•°å’Œè¾“å‡ºï¼Œå®šä¹‰å‘å‰ä¼ æ’­è¿‡ç¨‹
x = tf.placeholder(tf.float32, shape=(None,2), name='x')
y = tf.placeholder(tf.float32, shape=(None,2), name='y')

w1 = tf.Variable(tf.random_normal((2,3),stddev=1,seed=1))
b1 = tf.Variable(tf.random_normal((3,),stddev=1,seed=1))
w2 = tf.Variable(tf.random_normal((3,2),stddev=1,seed=1))
b2 = tf.Variable(tf.random_normal((2,),stddev=1,seed=1))

l1 = tf.nn.sigmoid(tf.add(tf.matmul(x,w1), b1))
y_ = tf.add(tf.matmul(l1,w2), b2)

#å®šä¹‰æŸå¤±å‡½æ•°åŠåå‘ä¼ æ’­æ–¹æ³•
loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_)) 
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)
#train_step=tf.train.AdamOptimizer(0.001).minimize(loss)

#ç”Ÿæˆä¼šè¯ï¼Œè®­ç»ƒSTEPSè½®
with tf.Session() as sess:
    init_op=tf.global_variables_initializer()
    sess.run(init_op)

    #è®­ç»ƒæ¨¡å‹
    STEPS = 30000
    for i in range(STEPS):
        start = (i*BATCH_SIZE) % 32
        end = start + BATCH_SIZE
        sess.run(train_step,feed_dict={x:X[start:end], y:Y[start:end]})
        if i % 500 ==0:
            total_loss = sess.run(loss,feed_dict={x:X, y:Y})
            print("After %d training steps, loss on all data is %g"%(i,total_loss))

    pred_y = sess.run(y_, feed_dict={x:X})
    pred_y = np.piecewise(pred_y, [pred_y<0, pred_y>0], [0, 1])

    l, r = X[:, 0].min() - 1, X[:, 0].max() + 1
    b, t = X[:, 1].min() - 1, X[:, 1].max() + 1
    n = 500
    grid_x, grid_y = np.meshgrid(np.linspace(l, r, n), np.linspace(b, t, n))
    samples = np.column_stack((grid_x.ravel(), grid_y.ravel()))

    grid_z = sess.run(y_, feed_dict={x:samples})
    grid_z = grid_z.reshape(-1, 2)[:,0]
    grid_z = np.piecewise(grid_z, [grid_z<0, grid_z>0], [0, 1])
    grid_z = grid_z.reshape(grid_x.shape)
    mp.figure('Logistic Classification', facecolor='lightgray')
    mp.title('Logistic Classification', fontsize=20)
    mp.xlabel('x', fontsize=14)
    mp.ylabel('y', fontsize=14)
    mp.tick_params(labelsize=10)
    mp.pcolormesh(grid_x, grid_y, grid_z, cmap='gray')
    mp.scatter(X[:, 0], X[:, 1], c=Y[:,0], cmap='brg', s=80)
    mp.show()
```

ç”±ç¥ç»ç½‘ç»œçš„å®ç°ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œæ€»å…± è®­ç»ƒ 30000 è½®ã€‚ æ¯è½®ä» X çš„æ•°æ®é›†å’Œ Y çš„æ ‡ç­¾ä¸­æŠ½å–ç›¸å¯¹åº”çš„ä» start å¼€å§‹åˆ° end ç»“æŸä¸ªç‰¹å¾å€¼ å’Œ æ ‡ç­¾ å–‚å…¥ç¥ç»ç½‘ç»œã€‚ ç”¨ sess.run æ±‚å‡º lossï¼Œ æ¯ 500 è½®æ‰“å°ä¸€æ¬¡ loss å€¼ ã€‚ç»è¿‡ 3000 è½®å æˆ‘ä»¬æ‰“å°å‡º æœ€ç»ˆè®­ç»ƒå¥½çš„ å‚æ•° w1 ã€ w2 ã€‚

## å›¾åƒå¤„ç†ä¸CNN

### è®¡ç®—æœºçœ¼ä¸­çš„å›¾åƒ

åœ¨è®¡ç®—æœºä¸­ï¼Œå¯¹äºå›¾åƒå­˜å‚¨æ˜¯é€šè¿‡çŸ©é˜µæ¥å­˜å‚¨çš„ã€‚ç…§ç‰‡åˆ†ä¸ºé»‘ç™½å’Œå½©è‰²ã€‚åœ¨å›¾åƒé‡Œæˆ‘ä»¬ç›¸åº”çš„æœ‰ç°åº¦å›¾å’Œå½©è‰²å›¾ã€‚

å¯¹äºç°åº¦å›¾åƒï¼Œç”±äºåªæœ‰æ˜æš—çš„åŒºåˆ«ï¼Œå› æ­¤åªéœ€è¦ä¸€ä¸ªæ•°å­—å°±å¯ä»¥è¡¨ç¤ºå‡ºä¸åŒçš„ç°åº¦ã€‚é€šå¸¸ç”¨0è¡¨ç¤ºæœ€æš—çš„é»‘è‰²ï¼Œ255è¡¨ç¤ºæœ€äº®çš„ç™½è‰²ï¼Œä»‹äº0å’Œ255ä¹‹é—´çš„æ•´æ•°åˆ™è¡¨ç¤ºä¸åŒæ˜æš—ç¨‹åº¦çš„ç°è‰²ã€‚

å¯¹äºå½©è‰²å›¾åƒï¼Œæˆ‘ä»¬ç”¨ï¼ˆR,G,Bï¼‰ä¸‰ä¸ªæ•°å­—æ¥è¡¨ç¤ºä¸€ä¸ªé¢œè‰²ï¼Œä»–ä»¬è¡¨ç¤ºç”¨çº¢ï¼ˆRï¼‰ã€ç»¿ï¼ˆGï¼‰ã€è“ï¼ˆBï¼‰ä¸‰ç§åŸºæœ¬é¢œè‰²å åŠ åçš„é¢œè‰²ã€‚å¯¹äºæ¯ç§åŸºæœ¬é¢œè‰²ï¼Œæˆ‘ä»¬ç”¨0åˆ°255ä¹‹é—´çš„æ•´æ•°è¡¨ç¤ºè¿™ä¸ªé¢œè‰²åˆ†é‡çš„æ˜æš—ç¨‹åº¦ã€‚

![img](images\clip_image003.png)

ä¸‰ä¸ªæ•°å­—ä¸­å¯¹åº”çš„æŸç§åŸºæœ¬é¢œè‰²çš„æ•°å­—è¶Šå¤§ï¼Œè¡¨ç¤ºè¯¥åŸºæœ¬é¢œè‰²çš„æ¯”ä¾‹è¶Šå¤§ï¼Œä¾‹å¦‚ï¼ˆ255,0,0ï¼‰è¡¨ç¤ºçº¯çº¢è‰²ï¼Œï¼ˆ0,255,0ï¼‰è¡¨ç¤ºçº¯ç»¿è‰²ï¼Œï¼ˆ135,206,255ï¼‰è¡¨ç¤ºå¤©è“è‰²ã€‚

ä¸€å¼ å½©è‰²å›¾ç‰‡æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç”±æ•´æ•°ç»„æˆçš„ç«‹æ–¹ä½“é˜µåˆ—æ¥è¡¨ç¤ºã€‚æˆ‘ä»¬ç§°è¿™æ ·çš„ç«‹æ–¹ä½“æ’åˆ—çš„æ•°å­—é˜µåˆ—ä¸ºä¸‰é˜¶å¼ é‡ï¼ˆtensorï¼‰ã€‚è¿™ä¸ªä¸‰é˜¶å¼ é‡çš„é•¿åº¦ä¸å®½åº¦å°±æ˜¯å›¾ç‰‡çš„åˆ†è¾¨ç‡ï¼Œé«˜åº¦ä¸º3.å¯¹äºæ•°å­—å›¾åƒè€Œè¨€ï¼Œä¸‰é˜¶å¼ é‡çš„é«˜åº¦ä¹Ÿæˆä¸ºé€šé“ï¼ˆchannelï¼‰æ•°ï¼Œå› æ­¤æˆ‘ä»¬è¯´å½©è‰²å›¾åƒæœ‰3ä¸ªé€šé“ã€‚çŸ©é˜µå¯ä»¥çœ‹æˆæ˜¯é«˜åº¦ä¸º1çš„ä¸‰é˜¶å¼ é‡ã€‚

 

### å›¾åƒç‰¹å¾æ¦‚è¿°

åœ¨æ·±åº¦å­¦ä¹ å‡ºç°ä¹‹å‰ï¼Œå›¾åƒç‰¹å¾çš„è®¾è®¡ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­ä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¯¾é¢˜ï¼Œåœ¨è¿™ä¸ªé¢†åŸŸå‘å±•åˆæœŸï¼Œäººä»¬æ‰‹å·¥è®¾è®¡äº†å„ç§å›¾åƒç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯ä»¥æè¿°å›¾åƒçš„é¢œè‰²ã€è¾¹ç¼˜ã€çº¹ç†ç­‰æ€§è´¨ï¼Œç»“åˆæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œèƒ½è§£å†³ç‰©ä½“è¯†åˆ«å’Œç‰©ä½“æ£€æµ‹ç­‰å®é™…é—®é¢˜ã€‚

æ—¢ç„¶å›¾åƒåœ¨è®¡ç®—æœºä¸­å¯ä»¥è¡¨ç¤ºæˆä¸‰é˜¶å¼ é‡ï¼Œé‚£ä¹ˆä»å›¾åƒä¸­æå–ç‰¹å¾ä¾¿æ˜¯å¯¹è¿™ä¸ªä¸‰é˜¶å¼ é‡è¿›è¡Œè¿ç®—çš„è¿‡ç¨‹ã€‚å…¶ä¸­éå¸¸é‡è¦çš„ä¸€ç§è¿ç®—å°±æ˜¯å·ç§¯ã€‚

### å·ç§¯è¿ç®—

#### å·ç§¯å®šä¹‰

å·ç§¯æ˜¯ä¸¤ä¸ªå˜é‡åœ¨æŸèŒƒå›´å†…ç›¸ä¹˜åæ±‚å’Œçš„ç»“æœã€‚

#### ä¸€ç»´å·ç§¯

å·ç§¯è¿ç®—çš„ç‰©ç†æ„ä¹‰ï¼šä¸€ä¸ªå‡½æ•°ï¼ˆå¦‚ï¼šå•ä½å“åº”ï¼‰åœ¨å¦ä¸€ä¸ªå‡½æ•°ï¼ˆå¦‚:è¾“å…¥ä¿¡å·ï¼‰ä¸Šçš„åŠ æƒå åŠ 

æœ‰ä¸¤ä¸ªç¦»æ•£ä¿¡å·

å¾…å·ç§¯ä¿¡å· X=[1,2,3,0,1,0]ï¼Œ

å·ç§¯æ ¸ H=[1,2,1]

å·ç§¯è¿ç®— Y = X * H

â€‹         ![img](images\clip_image005.jpg)

**valid**

è‡ªå§‹è‡³ç»ˆå·ç§¯æ ¸éƒ½åœ¨â€œä¿¡å·å†…â€

æœ€åå¾—åˆ°çš„ç»“æœçš„é•¿åº¦ä¼šå°äºå·ç§¯ä¿¡å·çš„é•¿åº¦

![img](images\clip_image006.jpg) 

**same**

å·ç§¯æ ¸çš„ä¸­å¿ƒåˆšå¥½æ˜¯ä»å¾…å·ç§¯ä¿¡å·çš„ç¬¬ä¸€ä¸ªå…ƒç´ â€œåˆ’â€åˆ°æœ€åä¸€ä¸ªå…ƒç´ å·ç§¯ç»“æœçš„é•¿åº¦å’Œå¾…å·ç§¯ä¿¡å·é•¿åº¦ä¸€æ ·

![img](images\clip_image007.jpg) 

**full** 

ä»å·ç§¯æ ¸çš„æœ€åä¸€ä¸ªå…ƒç´ å¼€å§‹ï¼Œç›´åˆ°ç¬¬ä¸€ä¸ªå…ƒç´ åˆ°ä¸å¾…å·ç§¯ä¿¡å·ç¬¬ä¸€ä¸ªå…ƒç´ å¯¹é½å·ç§¯ç»“æœçš„é•¿åº¦æ˜¯n+m-1![img](images\clip_image008.jpg) 

#### äºŒç»´å·ç§¯

å›¾åƒæ•°æ®æ˜¯5x5çš„äºŒç»´çŸ©é˜µï¼Œä½¿ç”¨ä¸€ä¸ª3x3çš„å·ç§¯æ ¸ï¼Œä»å·¦åˆ°å³ä»ä¸Šåˆ°ä¸‹æ»‘åŠ¨ã€‚æ»‘åŠ¨çš„è¿‡ç¨‹ç§°ä¸ºstrideï¼Œä¸€ä¸ªå·ç§¯å±‚æœ‰ä¸¤ä¸ªstrideï¼Œåˆ†åˆ«ä»ä¸Šåˆ°ä¸‹ï¼Œä»å·¦åˆ°å³ï¼Œæ­¥é•¿ä¸€èˆ¬è®¾å®šä¸º1æˆ–2ã€‚

![img](images\clip_image010.jpg) 

#### åˆ©ç”¨å·ç§¯æå–å›¾åƒç‰¹å¾

å·ç§¯è¿ç®—åœ¨å›¾åƒå¤„ç†ä¸­åº”ç”¨ååˆ†å¹¿æ³›ï¼Œè®¸å¤šå›¾åƒç‰¹å¾æå–æ–¹æ³•éƒ½ä¼šç”¨åˆ°å·ç§¯ã€‚ä»¥ç°åº¦å›¾ä¸ºä¾‹ï¼Œï¼Œæˆ‘ä»¬çŸ¥é“åœ¨è®¡ç®—æœºä¸­ï¼Œä¸€ä¸ªç°åº¦å›¾åƒè¢«è¡¨ç¤ºä¸ºä¸€ä¸ªæ•´æ•°çŸ©é˜µï¼Œå¦‚æœæˆ‘ä»¬ç”¨ä¸€ä¸ªå½¢çŠ¶è¾ƒå°çš„çŸ©é˜µå’Œè¿™ä¸ªå›¾åƒçŸ©é˜µåšå·ç§¯è¿ç®—ï¼Œå°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ–°çš„çŸ©é˜µï¼Œè¿™ä¸ªæ–°çš„çŸ©é˜µå¯ä»¥çœ‹ä½œæ˜¯ä¸€å‰¯æ–°çš„å›¾åƒï¼Œæ¢å¥è¯è¯´ï¼Œé€šè¿‡å·ç§¯è¿ç®—ï¼Œæˆ‘ä»¬å¯ä»¥å°†åŸå›¾åƒå˜æ¢ä¸ºä¸€å‰¯æ–°çš„å›¾åƒã€‚è¿™å¹…æ–°å›¾åƒæ¯”åŸå›¾åƒæ›´æ¸…æ¥šåœ°è¡¨ç¤ºäº†æŸäº›æ€§è´¨ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠå®ƒçœ‹åšåŸå›¾åƒçš„ä¸€ä¸ªç‰¹å¾ã€‚

è¿™é‡Œç”¨åˆ°çš„å°çŸ©é˜µå°±ç§°ä¸ºå·ç§¯æ ¸ï¼ˆconvolution lernelï¼‰ï¼Œé€šå¸¸ï¼Œå›¾åƒçŸ©é˜µä¸­çš„å…ƒç´ éƒ½æ˜¯ä»‹äº0åˆ°255çš„æ•´æ•°ï¼Œä½†å·ç§¯æ ¸ä¸­çš„å…ƒç´ å¯ä»¥æ˜¯ä»»æ„å®æ•°ã€‚

é€šè¿‡å·ç§¯ï¼Œæˆ‘ä»¬å¯ä»¥ä»å›¾åƒä¸­æå–è¾¹ç¼˜ç‰¹å¾ï¼Œåœ¨æ²¡æœ‰è¾¹ç¼˜çš„æ¯”è¾ƒå¹³å¦çš„åŒºåŸŸï¼Œå›¾åƒçš„åƒç´ å€¼çš„å˜åŒ–è¾ƒå°ï¼Œè€Œæ¨ªå‘è¾¹ç¼˜ä¸Šä¸‹ä¸¤ä¾§çš„åƒç´ å€¼ å·®å¼‚æ˜æ˜¾ï¼Œç«–å‘è¾¹ç¼˜å·¦å³ä¸¤ä¾§çš„åƒç´ ä¹Ÿä¼šæœ‰è¾ƒå¤§å·®åˆ«ã€‚

![img](images\clip_image013.jpg)

å¦‚ä¸Šå›¾ï¼Œæˆ‘ä»¬ç”¨1ã€0ã€-1 ç»„æˆçš„å·ç§¯æ ¸ä¸åŸå›¾åƒè¿›è¡Œå·ç§¯è¿ç®—ï¼Œå¯ä»¥ä»å›¾åƒä¸­æå–å‡ºç«–å‘è¾¹ç¼˜ã€‚

![img](images\clip_image015.jpg)

å¦‚ä¸Šå›¾ï¼Œæˆ‘ä»¬ç”¨ä¸‰è¡Œ1,0ï¼Œ-1ç»„æˆçš„å·ç§¯æ ¸ï¼Œä»å›¾ä¸­æå–å‡ºäº†æ¨ªå‘è¾¹ç¼˜ã€‚

äº‹å®ä¸Šï¼Œè¿™ä¸¤ä¸ªå·ç§¯æ ¸åˆ†åˆ«è®¡ç®—äº†åŸå›¾åƒä¸Šæ¯ä¸ª3*3åŒºåŸŸå†…å·¦å³åƒç´ æˆ–è€…ä¸Šä¸‹åƒç´ çš„å·®å€¼ï¼ˆä¸ºäº†å°†è¿ç®—ç»“æœä»¥å›¾åƒçš„å½¢å¼æ˜¾ç¤ºå‡ºæ¥ï¼Œæˆ‘ä»¬å¯¹è¿ç®—ç»“æœå»äº†ç»å¯¹å€¼ï¼‰ï¼Œé€šè¿‡è¿™æ ·çš„è¿ç®—ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä»å›¾åƒä¸Šæå–ä¸åŒçš„è¾¹ç¼˜ç‰¹å¾ã€‚

### å·ç§¯ç¥ç»ç½‘ç»œCNNåŸºæœ¬ç»“æ„

Alex Net ç¥ç»ç½‘ç»œ

![img](images\clip_image017.png)

ä¸Šå›¾ä¸ºAlex Net ç¥ç»ç½‘ç»œçš„ä¸»ä½“éƒ¨åˆ†ï¼Œä¸»ä½“éƒ¨åˆ†æœ‰5ä¸ªå·ç§¯å±‚å’Œ3ä¸ªå…¨è¿æ¥å±‚ç»„æˆ

5ä¸ªå·ç§¯å±‚ä½äºç½‘ç»œçš„æœ€å‰ç«¯ï¼Œä¾æ¬¡**å¯¹å›¾åƒè¿›è¡Œå˜æ¢ä»¥æå–ç‰¹å¾**ï¼›

æ¯ä¸ªå·ç§¯å±‚ä¹‹åéƒ½æœ‰ä¸€ä¸ª**ReLU**éçº¿æ€§æ¿€æ´»å±‚**å®Œæˆéçº¿æ€§å˜æ¢ï¼›**

ç¬¬ä¸€ã€äºŒã€äº”ä¸ªå·ç§¯å±‚ä¹‹åè¿æ¥æœ‰æœ€å¤§æ± åŒ– å±‚ï¼Œç”¨ä»¥**é™ä½ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡**ã€‚

ç»è¿‡5ä¸ªå·ç§¯å±‚ä»¥åŠç›¸è¿çš„éçº¿æ€§æ¿€æ´»å±‚ä¸æ± åŒ–å±‚ä¹‹åï¼Œç‰¹å¾å›¾è¢«è½¬æ¢ä¸º4096ç»´ç‰¹å¾å‘é‡ï¼Œåœ¨ç»è¿‡ä¸¤æ¬¡å…¨è¿æ¥å±‚å’ŒReLUå±‚å˜æ¢ä¹‹åï¼Œæˆä¸ºæœ€ç»ˆçš„ç‰¹å¾å‘é‡ï¼Œåœ¨ç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚å’Œä¸€ä¸ªsoftmaxå½’ä¸€åŒ–æŒ‡æ•°å±‚ä¹‹åï¼Œå°±å¾—åˆ°äº†å¯¹å›¾ç‰‡æ‰€å±ç±»å‹çš„é¢„æµ‹ã€‚

#### å·ç§¯å±‚

ç¥ç»ç½‘ç»œä¸­çš„å·ç§¯å±‚å°±æ˜¯ç”¨å·ç§¯è¿ç®—å¯¹åŸå§‹å›¾åƒæˆ–è€…ä¸Šä¸€å±‚çš„ç‰¹å¾è¿›è¡Œå˜æ¢çš„å±‚ã€‚åœ¨å‰è¾¹çš„å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†è¾¹ç¼˜ç‰¹å¾çš„æå–ï¼ŒçŸ¥é“ä¸€ç§ç‰¹å®šçš„å·ç§¯æ ¸å¯ä»¥å¯¹å›¾åƒè¿›è¡Œä¸€ç§ç‰¹å®šçš„å˜æ¢ï¼Œä»è€Œæå–å‡ºæŸç§ç‰¹å®šçš„ç‰¹å¾ï¼Œå¦‚æ¨ªå‘è¾¹ç¼˜æˆ–è€…ç«–å‘è¾¹ç¼˜ã€‚

åœ¨ä¸€ä¸ªå·ç§¯å±‚ä¸­ï¼Œä¸ºäº†ä»å›¾åƒä¸­æå–å¤šç§å½¢å¼çš„ç‰¹å¾ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨å¤šä¸ªå·ç§¯æ ¸å¯¹è¾“å…¥çš„å›¾åƒè¿›è¡Œä¸åŒçš„å·ç§¯æ“ä½œã€‚ä¸€ä¸ªå·ç§¯æ ¸å¯ä»¥å¾—åˆ°ä¸€ä¸ªé€šé“ä¸º1çš„ä¸‰é˜¶å¼ é‡ï¼Œï¼Œå¤šä¸ªå·ç§¯æ ¸å°±å¯ä»¥å¾—åˆ°å¤šä¸ªé€šé“ä¸º1çš„ä¸‰é˜¶å¼ é‡ç»“æœã€‚æˆ‘ä»¬æŠŠè¿™äº›ç»“æœä½œä¸ºä¸åŒçš„é€šé“ç»„åˆèµ·æ¥ï¼Œå°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ–°çš„ä¸‰é˜¶å¼ é‡ï¼Œè¿™ä¸ªä¸‰é˜¶å¼ é‡çš„é€šé“æ•°å°±ç­‰äºæˆ‘ä»¬ä½¿ç”¨çš„å·ç§¯æ ¸çš„ä¸ªæ•°ã€‚ç”±äºæ¯ä¸€ä¸ªé€šé“éƒ½æ˜¯ä»åŸå›¾åƒä¸­æå–çš„ä¸€ç§ç‰¹å¾ï¼Œæˆ‘ä»¬ä¹Ÿå°†è¿™ä¸ªä¸‰é˜¶å¼ é‡ç§°ä¸ºç‰¹å¾å›¾ï¼ˆfeature mapï¼‰ã€‚è¿™ä¸ªç‰¹å¾å›¾å°±æ˜¯å·ç§¯å±‚çš„æœ€ç»ˆè¾“å‡ºã€‚

ç‰¹å¾å›¾ä¸å½©è‰²å›¾åƒéƒ½æ˜¯ä¸‰é˜¶å¼ é‡ï¼Œä¹Ÿéƒ½æœ‰è‹¥å¹²ä¸ªé€šé“ã€‚å› æ­¤å·ç§¯å±‚ä¸ä»…å¯ä»¥ä½œç”¨äºå›¾åƒï¼Œä¹Ÿå¯ä»¥ä½œç”¨äºå…¶ä»–è¾“å‡ºçš„ç‰¹å¾å›¾ã€‚é€šå¸¸ï¼Œä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚ä¼šä»¥å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè€Œä¹‹åçš„å·ç§¯å±‚ä¼šä»¥å‰é¢çš„ç‰¹å¾å›¾ä¸ºè¾“å…¥ã€‚

#### éçº¿æ€§æ¿€æ´»å±‚

é€šå¸¸æˆ‘ä»¬éœ€è¦åœ¨æ¯ä¸ªå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚åé¢éƒ½è¿æ¥ä¸€ä¸ªéçº¿æ€§æ¿€æ´»å±‚ï¼ˆnon-linear activation layerï¼‰ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå…¶å®ä¸ç®¡æ˜¯å·ç§¯è¿ç®—è¿˜æ˜¯å…¨è¿æ¥å±‚ä¸­çš„è¿ç®—ï¼Œä»–ä»¬éƒ½æ˜¯è‡ªå˜é‡çš„ä¸€æ¬¡å‡½æ•°ï¼Œå³æ‰€è°“çš„çº¿æ€§å‡½æ•°ï¼ˆlinear functionï¼‰ã€‚çº¿æ€§å‡½æ•°æœ‰ä¸€ä¸ªæ€§è´¨ï¼šè‹¥å¹²çº¿æ€§è®¡ç®—çš„å¤åˆä»ç„¶æ˜¯çº¿æ€§çš„ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬åªæ˜¯å°†å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚ç›´æ¥å †å èµ·æ¥ï¼Œï¼Œé‚£ä¹ˆå®ƒä»¬å¯¹è¾“å…¥å›¾ç‰‡äº§ç”Ÿçš„æ•ˆæœå°±å¯ä»¥è¢«ä¸€ä¸ªå…¨è¿æ¥å±‚æ›¿ä»£ã€‚è¿™æ ·ä¸€æ¥ï¼Œè™½ç„¶æˆ‘ä»¬å †å äº†å¾ˆå¤šå±‚ï¼Œä½†å¯¹æ¯ä¸€å±‚çš„å˜æ¢æ•ˆæœå®é™…ä¸Šè¢«åˆå¹¶åˆ°äº†ä¸€èµ·ã€‚è€Œå¦‚æœæˆ‘ä»¬åœ¨æ¯æ¬¡çº¿æ€§è¿ç®—åï¼Œå†è¿›è¡Œä¸€æ¬¡éçº¿æ€§è¿ç®—ï¼Œé‚£ä¹ˆæ¯æ¬¡å˜æ¢çš„æ•ˆæœå°±å¯ä»¥ä¿ç•™ã€‚éçº¿æ€§æ¿€æ´»å±‚çš„å½¢å¼ä¸å¾ˆå¤šç§ï¼Œå®ƒä»¬çš„åŸºæœ¬å½¢å¼æ˜¯å…ˆé€‰å®šæŸç§éçº¿æ€§å‡½æ•°ï¼Œç„¶åå¯¹è¾“å…¥ç‰¹å¾å›¾æˆ–è€…ç‰¹å¾å‘é‡çš„æ¯ä¸€ä¸ªå…ƒç´ åº”ç”¨è¿™ç§éçº¿æ€§å‡½æ•°ï¼Œå¾—åˆ°è¾“å‡ºã€‚

å¸¸è§çš„éçº¿æ€§å‡½æ•°æœ‰ï¼š

l  é€»è¾‘å‡½æ•°ï¼ˆlogistic functionï¼‰sigmoid

![img](images/clip_image036.png)

![img](images/clip_image037.png)

l  åŒæ›²æ­£åˆ‡å‡½æ•°ï¼ˆhyperbolic tangent functionï¼‰

![img](images/clip_image038.png)

![img](images/clip_image039.png)

l  ä¿®æ­£çº¿æ€§å‡½æ•°ï¼ˆrectified linear functionï¼‰

![img](images/clip_image040.png)

![img](images/clip_image041.png)

å‰ä¸¤è€…sigmoid/tanhæ¯”è¾ƒå¸¸è§äºå…¨è¿æ¥å±‚ï¼Œåè€…ReLUå¸¸è§äºå·ç§¯å±‚ã€‚

æ¿€æ´»å‡½æ•°æ˜¯ç”¨æ¥åŠ å…¥éçº¿æ€§å› ç´ çš„ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥ä»»æ„é€¼è¿‘ä»»ä½•éçº¿æ€§å‡½æ•°ï¼Œæé«˜ç¥ç»ç½‘ç»œå¯¹æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè§£å†³çº¿æ€§æ¨¡å‹æ‰€ä¸èƒ½è§£å†³çš„é—®é¢˜ï¼Œè¿™æ ·ç¥ç»ç½‘ç»œå°±å¯ä»¥åº”ç”¨åˆ°ä¼—å¤šçš„éçº¿æ€§æ¨¡å‹ä¸­ã€‚

ä»¥ReLUå±‚ä¸ºä¾‹ï¼Œå¯¹äºè¾“å…¥çš„ç‰¹å¾å‘é‡æˆ–ç‰¹å¾å›¾ï¼Œä»–ä¼šå°†å…¶ä¸­å°äºé›¶çš„å…ƒç´ å˜æˆé›¶ï¼Œè€Œå…¶ä»–å…ƒç´ çš„å€¼ä¿æŒä¸å˜ï¼Œå°±å¾—åˆ°äº†è¾“å‡ºã€‚

![https://static.oschina.net/uploads/space/2018/0210/003400_MYqn_876354.png](images/clip_image042.png)

å› ä¸ºReLUçš„è®¡ç®—éå¸¸ç®€å•ï¼Œæ‰€ä»¥å®ƒçš„è®¡ç®—é€Ÿåº¦å¾€å¾€æ¯”å…¶ä»–éçº¿æ€§æ¿€æ´»å±‚å¿«å¾ˆå¤šï¼ŒåŠ ä¹‹å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœä¹Ÿå¾ˆå¥½ï¼Œå› æ­¤åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­è¢«å¹¿æ³›åœ°ä½¿ç”¨ã€‚

####  æ± åŒ–å±‚

åœ¨è®¡ç®—å·ç§¯æ—¶ï¼Œæˆ‘ä»¬ä¼šç”¨å·ç§¯æ ¸æ»‘è¿‡å›¾åƒæˆ–è€…ç‰¹å¾å›¾çš„æ¯ä¸€ä¸ªåƒç´ ã€‚å¦‚æœå›¾åƒæˆ–è€…ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡å¾ˆå¤šï¼Œé‚£ä¹ˆå·ç§¯çš„è®¡ç®—é‡å°±ä¼šå¾ˆå¤§ã€‚ä¸ºäº†è§£å†³ è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸åœ¨å‡ ä¸ªå·ç§¯å±‚ä¹‹åæ’å…¥æ± åŒ–å±‚ï¼ˆpooling layerï¼‰ï¼Œå·²é™ä½ç‰¹å¾å›¾çš„åˆ†è¾¨ç‡ã€‚

æ± åŒ–å±‚çš„åŸºæœ¬æ“ä½œæ­¥éª¤å¦‚ä¸‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ç‰¹å¾å›¾æŒ‰é€šé“åˆ†å¼€ï¼Œå¾—åˆ°è‹¥å¹²ä¸ªçŸ©é˜µã€‚å¯¹äºæ¯ä¸ªçŸ©é˜µï¼Œæˆ‘ä»¬å°†å…¶åˆ‡å‰²æˆè‹¥å¹²å¤§å°ç›¸ç­‰çš„æ­£æ–¹å½¢å°å—ã€‚å¦‚ä¸‹å›¾ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ª4x4çš„çŸ©é˜µåˆ†å‰²æˆ4ä¸ªæ­£æ–¹å½¢åŒºå—ï¼Œæ¯ä¸ªåŒºå—çš„å¤§å°ä¸º2x2æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸€ä¸ªåŒºå—å–æœ€å¤§å€¼æˆ–è€…å¹³å‡å€¼ï¼Œå¹¶å°†ç»“æœç»„æˆä¸€ä¸ªæ–°çš„çŸ©é˜µã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰é€šé“çš„ç»“æœçŸ©é˜µæŒ‰åŸé¡ºåºå †å èµ·æ¥å½¢æˆä¸€ä¸ªä¸‰é˜¶å¼ é‡ï¼Œè¿™ä¸ªä¸‰é˜¶å¼ é‡å°±æ˜¯æ± åŒ–å±‚çš„è¾“å‡ºã€‚**å¯¹äºæ¯ä¸€ä¸ªåŒºå—å–æœ€å¤§å€¼çš„æ± åŒ–å±‚ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæœ€å¤§æ± åŒ–å±‚ï¼ˆmax poolingï¼‰ï¼Œè€Œå–å¹³å‡å€¼çš„æ± åŒ–å±‚æˆä¸ºå¹³å‡æ± åŒ–å±‚ï¼ˆaverage pooling layerï¼‰ã€‚**

![img](images/clip_image044.png)

åœ¨ç»è¿‡æ± åŒ–åï¼Œç‰¹å¾å›¾çš„é•¿å’Œå®½éƒ½ä¼šå‡å°åˆ°åŸæ¥çš„1/2ï¼Œç‰¹å¾å›¾ä¸­çš„å…ƒç´ æ•°ç›®å‡å°åˆ°åŸæ¥çš„1/4ã€‚é€šå¸¸æˆ‘ä»¬ä¼šåœ¨å·ç§¯å±‚ä¹‹åå¢åŠ æ± åŒ–å±‚ã€‚è¿™æ ·ï¼Œåœ¨ç»è¿‡è‹¥å¹²å·ç§¯ã€æ± åŒ–å±‚çš„ç»„åˆä¹‹åï¼Œåœ¨ä¸è€ƒè™‘é€šé“æ•°çš„æƒ…å†µä¸‹ï¼Œç‰¹å¾å›¾çš„åˆ†è¾¨ç‡å°±ä¼šè¿œå°äºè¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡ï¼Œå¤§å¤§å‡å°äº†å¯¹è®¡ç®—é‡å’Œå‚æ•°æ•°é‡çš„éœ€æ±‚ã€‚

#### å…¨è¿æ¥å±‚  FC (full connection)

å…¨è¿æ¥å±‚åœ¨æ•´ä¸ªå·ç§¯ç¥ç»ç½‘ç»œä¸­èµ·åˆ°â€œåˆ†ç±»å™¨â€çš„ä½œç”¨ï¼Œå³é€šè¿‡å·ç§¯ã€æ¿€æ´»å‡½æ•°ã€æ± åŒ–ç­‰æ·±åº¦ç½‘ç»œåï¼Œå†ç»è¿‡å…¨è¿æ¥å±‚å¯¹ç»“æœè¿›è¡Œè¯†åˆ«åˆ†ç±»ã€‚ï¼š

![img](images/clip_image002.png)

ç”±äºç¥ç»ç½‘ç»œæ˜¯å±äºç›‘ç£å­¦ä¹ ï¼Œåœ¨æ¨¡å‹è®­ç»ƒæ—¶ï¼Œæ ¹æ®è®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå¾—åˆ°å…¨è¿æ¥å±‚çš„æƒé‡ï¼ˆå¦‚é¢„æµ‹å­—æ¯Xçš„æ‰€æœ‰è¿æ¥çš„æƒé‡)

![img](images/clip_image045.png)

æœ€åè®¡ç®—å‡ºæ¥å­—æ¯Xçš„è¯†  åˆ«å€¼ä¸º0.92ï¼Œå­—æ¯Oçš„è¯†åˆ«å€¼ä¸º0.51ï¼Œåˆ™ç»“æœåˆ¤å®šä¸ºX

![img](images/clip_image046.png)

â€œå·ç§¯ç¥ç»ç½‘ç»œâ€ï¼ˆCNNï¼‰ç»“æ„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![img](images/clip_image047.png)



![img](images/clip_image048.png)

### è®¾è®¡å·ç§¯ç¥ç»ç½‘ç»œå®ç°æ‰‹å†™æ•°å­—è¯†åˆ«

#### mnist æ•°æ®é›†

mnist æ•°æ®é›† ï¼šåŒ…å« 7 ä¸‡å¼  é»‘åº•ç™½å­—æ‰‹å†™æ•°å­— å›¾ç‰‡ï¼Œ å…¶ä¸­ 55000 å¼ ä¸ºè®­ç»ƒé›†ï¼Œ5000 å¼ ä¸ºéªŒè¯é›†ï¼Œ 10000 å¼  ä¸ºæµ‹è¯•é›† ã€‚æ¯å¼ å›¾ç‰‡å¤§å°ä¸º 28*28 åƒç´ ï¼Œå›¾ç‰‡ä¸­ çº¯ é»‘è‰²åƒç´  å€¼ä¸º 0ï¼Œ çº¯ ç™½è‰²åƒç´ å€¼ä¸º 1 ã€‚æ•°æ®é›†çš„æ ‡ç­¾æ˜¯é•¿åº¦ä¸º 10 çš„ä¸€ç»´æ•°ç»„ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ ç´¢å¼•å·è¡¨ç¤ºå¯¹åº”æ•°å­—å‡ºç°çš„æ¦‚ç‡ ã€‚

tf.cast(x,dtype) å‡½æ•°è¡¨ç¤ºå°†å‚æ•° x è½¬æ¢ä¸ºæŒ‡å®š æ•°æ® ç±»å‹ ã€‚

tf.reduce_mean( x,axis å‡½æ•°è¡¨ç¤ºæ±‚å–çŸ©é˜µæˆ–å¼ é‡æŒ‡å®šç»´åº¦çš„å¹³å‡å€¼ã€‚ 

tf argmax(x,axis) å‡½æ•°è¡¨ç¤º è¿”å› æŒ‡å®šç»´åº¦ a xis ä¸‹ï¼Œå‚æ•° x ä¸­ æœ€å¤§å€¼ç´¢å¼•å· ã€‚

os.path.join å‡½æ•°è¡¨ç¤º æŠŠ å‚æ•° å­—ç¬¦ä¸²æŒ‰ç…§è·¯å¾„å‘½åè§„åˆ™æ‹¼æ¥ã€‚

å­—ç¬¦ä¸² split( å‡½æ•°è¡¨ç¤º æŒ‰ç…§æŒ‡å®š æ‹†åˆ†ç¬¦ å¯¹å­—ç¬¦ä¸²æ‹†åˆ† è¿”å›æ‹†åˆ†åˆ—è¡¨ ã€‚

#### å®ç°

```python
from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

# sample = minist.train.next_batch(1)
# sample[0]  28*28å›¾åƒ    ä¸€è¡Œ784åˆ—     è¾“å…¥
# sample[1]  10å…ƒç´ çš„æ•°ç»„               è¾“å‡º


#ç”Ÿæˆæƒé‡
def weight_variable(shape):
  initial = tf.random_normal(shape, stddev=0.1)
  return tf.Variable(initial)

#ç”Ÿæˆb
def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)

#å·ç§¯å±‚
def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding='SAME')

x = tf.placeholder("float", shape=[None, 784])
y_ = tf.placeholder("float", shape=[None, 10])

# nå¼  28*28*1çš„å›¾åƒ
x_image = tf.reshape(x, [-1,28,28,1])

# 32ä¸ª5*5*1çš„å·ç§¯æ ¸   
# å·ç§¯æ‰§è¡Œè¿‡åå¾—åˆ°28*28*1*32ï¼ˆ32ä¸ªé€šé“ï¼‰
W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
# æ± åŒ–è¿‡åï¼š 14*14*1*32
h_pool1 = max_pool_2x2(h_conv1)

# (14,14,32) * (5,5,32,64)   => (14*14*64)
W_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
# æ± åŒ–è¿‡åï¼š 7*7*64
h_pool2 = max_pool_2x2(h_conv2)

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])

W_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
#
# keep_prob = tf.placeholder("float")
# h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

# softmaxå±‚
W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv=tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)

with tf.Session() as sess:
    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    sess.run(tf.initialize_all_variables())
    for i in range(20000):
        batch = mnist.train.next_batch(50)
        if i%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={
                x:batch[0], y_: batch[1]})
            print("step %d, training accuracy %g"%(i, train_accuracy))
        train_step.run(feed_dict={x: batch[0], y_: batch[1]})

    print("test accuracy %g"%accuracy.eval(feed_dict={
        x: mnist.test.images, y_: mnist.test.labels}))
```

